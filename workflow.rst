Overall Workflow
================

To reduce the KOA DEIMOS data we used the following general workflow.

1.  Preparing Data.

    Once data has been identified and downloaded, the ``adap_reorg_setup.py`` script organizes it::

        adap_reorg_setup.py --symlink raw_data_reorg RAW_DATA RAW_DATA2 --report reorg_report.txt

    This organizes the raw data into what we call "datasets".  Theey are named 
    "*mask*/*observing config*/*date*".  The reorg script organizes them into ``complete`` or 
    ``incomplete`` directories based on whether or not the script determined there were enough files to
    reduce the dataset. The raw data is then placed into a ``raw`` subdirectory. For example::
    
        HDa06DB/600ZD_8000_GG455/2006-04-27/complete/raw
        HDa06BC/600ZD_8000_GG455/2006-04-26_2006-04-29/complete/raw
        cdf07b/600ZD_7899_GG400/2007-10-11/complete/raw

2.  Initial configuration

    The ADAP scripts automatically generate ``.pypeit`` files, ``.colalte1d`` files, and 
    ``.2dcoadd`` files. A default set of configurations params for each of these files is 
    stored in::

        config/default_pypeit_config
        config/default.colalte1d
        config/default_2dcoadd_config


3.  Prepare a Scorecard.
    The results of the reductions are sent to a Google Sheets spreadsheet called a scorecard.
    The raw data is generated by `scoreacard.py <scripts/scorecard.py>`_. This data is then uploaded
    to the spreadsheet by `update_gsheet_scorecard.py <scripts/scorecard.py>`_.  The spreadsheet is
    organized into tabs based on the first letter of the DEIMOS mask being reduced.


4.  Prepare Work Queues
    The ADAP scripts work off a queue iniitialized from the Scorecard spreadsheet.

    The reduction scripts use a special tab named ``WorkQueue``.  This must have 3 columns:
    ``dataset``, ``status``, and ``pod``. Before starting the reduction, fill the dataset
    column with the datasets that you want to reduce.  The scripts will then populate 
    ``status`` and ``pod`` as the scripts run.  When initializing, the jobs will only
    load the work queue with jobs that have a blank status.

    
5.  Start a work queue server in Nautilus. 
    The actual work queue is managed by a redis instance running in Nautilus. To
    start that instance run::

        kubectl create -f workqueue_deployment.yml

6.  Populate Queue with "init"::

        load_nautilus_redis_queue.sh  adap_2019_q init

    The ``init`` tells the first pod in the cloud to initialize the redis queue
    from the Google Spreadsheet.

    Alternatively, it is possible to directly populate the queue::

        load_nautilus_redis_queue.sh  adap_2019_q HDa06DB/600ZD_8000_GG455/2006-04-27 HDa06BC/600ZD_8000_GG455/2006-04-26_2006-04-29 cdf07b/600ZD_7899_GG400/2007-10-11

7.  Prepare the yaml for the reduction.

    There are a number of describe that run tasks from queues. They are run
    by yaml files in the ``nautilus_jobs`` directory. For reductions, the 
    script is called `reduce_from_queue.py <scripts/reduce_from_queue.py>`_ and it is run 
    by the yml file  `adap-reduce-from-queue.yml <nautilus_jobs/adap-reduce-from-queue.yml>`_.

    Before running the reduction the yaml file will need to be modified in the following ways:

        **parallelism** - This should be edited to the number of concurrent reductions you want to run. 
        I recommend 10 to 20.
               
        **pypeit branch** - In the ``args`` section of the yaml file, the bash commands run in the cloud are listed.
        There is a ``git checkout`` command that controls which branch on the PypeIt repository is
        used. Make sure this is set to the desired branch, tag, or commit.
        
        **redis url_** - The ``reduce_from_queue.py`` script needs to know how to reach the work queue server. To find this address run::
        
            kubectl -n pypeit get pods -o wide
        
        This will list the pods in the ``pypeit`` namespace. Find the one whose name starts with ``adap-workqueue``.
        Use this pod's IP in the redis url.
        
        **ephemeral storage** - The Nautilus jobs store the PypeIt reduction data onto ephemeral storage before uploading it to S3.
        Different sized datasets require differing amounts of data, anywhere from 40 - 150 GiB. Set this
        value for the size required by the datasets being reduced.

    Example edited yaml:

    .. parsed-literal::

        apiVersion: batch/v1
        kind: Job
        metadata:
        name: adap2019-reduce-from-queue
        spec:
        backoffLimit: 2
        **parallelism: 10**
        
        *. . .*

            containers:
            - name: reduce-worker
                args:
                - source /home/pypeitusr/pypeit_env/bin/activate; pip install redis boto3 google-api-python-client gspread;  
                cd /home/pypeitusr/PypeIt; git fetch; **git checkout b8455e6593aeb6a2d55bdd2d2ad3f62c5fbe4733;** 

                *. . .*

                python scripts/reduce_from_queue.py --rclone_conf config/rclone.conf --adap_root_dir /tmp/adap_root --scorecard_max_age 7 Scorecard/WorkQueue redis://**10.244.102.107**:6379 adap_2019 s3 -- -o;

                *. . .*

                resources:
                limits:
                    cpu: '1.5'
                    ephemeral-storage: **50Gi**
                    memory: 38Gi
                requests:
                    cpu: '1.5'
                    ephemeral-storage: **50Gi**
                    memory: 32Gi

                *. . .*
        

8.  Run the reduction::

        kubectl -n pypeit create -f adap-reduce-from-queue.yml

    As the reductions are running they can be monitored with:

    .. parsed-literal::

        kubectl -n pypeit get pods
        kubectl logs -f *pod name from get pods*

    They will also update their status in the ``WorkQueue`` tab in the Scorecard.


9.  Evaluate results, Customize Configuration

    The results of the reduction are stored in both google drive and Nautilus S3 under a path named
    "*dataset name*/complete/reduce". After evaluating the results of the reduction, it will probably
    be neccessary to customize the parameters for some specific datasets. This is downloaded
    with files in the `config/custom_pypeit <config/custom_pypeit>`_ directory and the `config/custom_params <config/custom_params>`_ directories.

    The files in the ``custom_params`` directories contain custom parameters for the PypeIt reduction, but still allow the `trimming_setup.py <scripts/trimming_setup.py>`_ 
    script to generate the file metadata section of the PypeIt file.  They are named after the dataset name, with ``/`` replaced with ``+``, and the ``_reduce..ini`` suffix.

    The files in the ``custom_pypeit`` directories contain the entire PypeIt input file. When they are run, the path to the raw data is changed but
    everything else remains the same. They are named after the dataset name,  with ``/`` replaced with ``+``, and the ``_reduce.pypeit`` suffix.

    For example see `KAKm1_600ZD_5500_BAL12_2020-02-25_reduce.ini <config/custom_params/KAKm1_600ZD_5500_BAL12_2020-02-25_reduce.ini>`_ 
    and `22_1b_830G_7800_OG550_2002-09-13_reduce.pypeit <config/custom_pypeit/22_1b_830G_7800_OG550_2002-09-13_reduce.pypeit>`_.


10. Collating and Coadding.
    
    After reduction, flux calibration and 1D coadding is done with the  `collate1d_from_queue.py <scripts/collate1d_from_queue.py>`_ script 
    run by the `adap-collate1d-queue.yml <nautilus_jobs/adap-collate1d-         queue.yml>`_ yaml file.  
    This should be modified in a similar way as described for the ``adap-reduce-from-queue.yml`` file.

    Similarly 2D coadding is done with the `coadd2d_from_queue.py <scripts/coadd2d_from_queue.py>`_ script run by the 
    `adap-coadd2d-queue.yml <nautilus_jobs/adap-coadd2d-queue.yml>`_ yaml file.

    The collate1d and coadd2d scripts use a different tab named ``coadd status`` to
    populate their work queues. This reflects that they are performed at a different level
    than the reductions. This tab this must have five columns: ``dataset`` ``Coadd 2D``, 
    ``Coadd 2D pod``, ``Coadd 1D``, and ``Coadd 1D pod``. The dataset column is used to initialize the queue as in the
    ``WorkQueue`` tab.  However the ``dataset`` column only has the prefix of the dataset name depending
    on the the level the coadd is performed at. For example::

        071715B1/1200G_8800_OG550
        074416A1/1200G_8800_OG550/2016-02-06_2016-02-07
        074416A1/1200G_8800_OG550/2016-03-11

    For ``071715B1`` all of the science frames from the ``1200_8800_OG550`` observing
    config were coadded together. But for ``074416A1``, the coadding was done at the date level.

    To run the coadding::
        
        kubectl create -f adap-collate1d-queue.yml

        kubectl create -f adap-coadd2d-queue.yml

    The results are stored directories named ``1D_Coadd``` and ``2D_Coadd`` in both Natilus S3 and Google Drive.
    For example::

        071715B1/1200G_8800_OG550/1D_Coadd
        071715B1/1200G_8800_OG550/2D_Coadd
        074416A1/1200G_8800_OG550/2016-02-06_2016-02-07/1D_Coadd
        074416A1/1200G_8800_OG550/2016-02-06_2016-02-07/2D_Coadd

    The ``.coadd2d`` files are usually generated by ``pypeit_setup_coadd2d``. However custom ones can be supplied
    in the `config/custom_coadd2d <config/custom_coadd2d/>`_ directory.

11. Organize files into a format useable by KOA

    The directory structure used during reduction and coadding is flattened out before being provided to KOA. This is done
    using two scripts: `local_archive.py <scripts/local_archive.py>`_ and `remote_archive.py <scripts/remote_archive.py>`_. 


    ``local_archive.py`` works on files on the local file system in the case where KOA is being provided with a files by sending a physical disk.
    It and can be run as::

        python ~/work/adap/scripts/local_archive.py raw_data_reorg/ --copy archive --report archive_test.report.`date --iso-8601=s`.txt  --subdirs egs_pr emit egs_blg egsmk1

    The above assumes ``raw_data_reorg`` is the ADAP directory heriarchy as stored in Nautilus S3, although the ``raw`` files are not required.
    The above script copies the neccessary files from the reduction and coadding results to the ``archive`` directory. The `archive_README <scripts/archive_README>`_ readme 
    is also copied to the ``archive`` directory and describes the files and metadata files sent to KOA.

    The ``remote_archive.py`` script was written to perform the same re-organization in Google Drive. It can be run in parallel however is
    limited by the 750 GiB / day upload quota imposed by Google. It is run by the `adap-create-archive.yml <nautilus_jobs/adap-create-archive.yml>`_
    yaml file. It may need customization for ephemeral storage size and redis url as described earlier.

    The work queue is managed differently for ``remote_archive.py``. Instead of being initialized by the scorecard, it is initialized directly with
    with ``load_nautilus_redis_queue.sh``. It will load status into a redis hash to determine if it succeeded or failed for a particular subdirectory. 
    For example::

        $ ./load_nautilus_redis_queue.sh adap_2019_archive_q emit10 egs_blg egsmk1

        $ kubectl -n pypeit create -f adap-create-archive.yml

        $ ./display_natilus_redis_hash.sh adap_2019_archive_status 

